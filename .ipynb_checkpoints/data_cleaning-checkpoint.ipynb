{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1b15426-3dfa-4dca-863f-3b15b28ceb2b",
   "metadata": {},
   "source": [
    "## Data Transformation from JSON to Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb452bf6-9bb4-4e95-a424-8a557339e46f",
   "metadata": {},
   "source": [
    "### All Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0baf9c78-0175-4ef9-a584-238e900fa30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode, col, arrays_zip\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import sum,avg,max\n",
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e53ea-51f0-4868-97d7-a62f8dab8974",
   "metadata": {},
   "source": [
    "### Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9ef91d-d6a0-47d7-b193-7df7a1d3f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .appName(\"data_cleaning\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4676a23-3e17-4975-8e21-f422402ac2fa",
   "metadata": {},
   "source": [
    "### Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8800514d-40ad-42b3-a775-9b24523f5dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = spark.read.option(\"multiline\",\"true\").json([\"./Data/JSON/drug-event-0033-of-0034.json\"])\n",
    "# json_data = spark.read.option(\"multiline\",\"true\").json(\"./Data/JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48bbac0f-4207-4e7f-8567-0666944e5617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- meta: struct (nullable = true)\n",
      " |    |-- disclaimer: string (nullable = true)\n",
      " |    |-- last_updated: string (nullable = true)\n",
      " |    |-- license: string (nullable = true)\n",
      " |    |-- results: struct (nullable = true)\n",
      " |    |    |-- limit: long (nullable = true)\n",
      " |    |    |-- skip: long (nullable = true)\n",
      " |    |    |-- total: long (nullable = true)\n",
      " |    |-- terms: string (nullable = true)\n",
      " |-- results: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- authoritynumb: string (nullable = true)\n",
      " |    |    |-- companynumb: string (nullable = true)\n",
      " |    |    |-- duplicate: string (nullable = true)\n",
      " |    |    |-- fulfillexpeditecriteria: string (nullable = true)\n",
      " |    |    |-- occurcountry: string (nullable = true)\n",
      " |    |    |-- patient: struct (nullable = true)\n",
      " |    |    |    |-- drug: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- actiondrug: string (nullable = true)\n",
      " |    |    |    |    |    |-- activesubstance: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- activesubstancename: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugadditional: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugadministrationroute: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugauthorizationnumb: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugbatchnumb: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugcharacterization: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugcumulativedosagenumb: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugcumulativedosageunit: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugdosageform: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugdosagetext: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugenddate: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugenddateformat: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugindication: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugintervaldosagedefinition: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugintervaldosageunitnumb: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugrecurreadministration: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugseparatedosagenumb: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugstartdate: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugstartdateformat: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugstructuredosagenumb: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugstructuredosageunit: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugtreatmentduration: string (nullable = true)\n",
      " |    |    |    |    |    |-- drugtreatmentdurationunit: string (nullable = true)\n",
      " |    |    |    |    |    |-- medicinalproduct: string (nullable = true)\n",
      " |    |    |    |    |    |-- openfda: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- application_number: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- brand_name: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- generic_name: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- manufacturer_name: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- nui: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- package_ndc: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- pharm_class_cs: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- pharm_class_epc: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- pharm_class_moa: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- pharm_class_pe: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- product_ndc: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- product_type: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- route: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- rxcui: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- spl_id: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- spl_set_id: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- substance_name: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- unii: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- patientagegroup: string (nullable = true)\n",
      " |    |    |    |-- patientonsetage: string (nullable = true)\n",
      " |    |    |    |-- patientonsetageunit: string (nullable = true)\n",
      " |    |    |    |-- patientsex: string (nullable = true)\n",
      " |    |    |    |-- patientweight: string (nullable = true)\n",
      " |    |    |    |-- reaction: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- reactionmeddrapt: string (nullable = true)\n",
      " |    |    |    |    |    |-- reactionmeddraversionpt: string (nullable = true)\n",
      " |    |    |    |    |    |-- reactionoutcome: string (nullable = true)\n",
      " |    |    |    |-- summary: struct (nullable = true)\n",
      " |    |    |    |    |-- narrativeincludeclinical: string (nullable = true)\n",
      " |    |    |-- primarysource: struct (nullable = true)\n",
      " |    |    |    |-- literaturereference: string (nullable = true)\n",
      " |    |    |    |-- qualification: string (nullable = true)\n",
      " |    |    |    |-- reportercountry: string (nullable = true)\n",
      " |    |    |-- primarysourcecountry: string (nullable = true)\n",
      " |    |    |-- receiptdate: string (nullable = true)\n",
      " |    |    |-- receiptdateformat: string (nullable = true)\n",
      " |    |    |-- receivedate: string (nullable = true)\n",
      " |    |    |-- receivedateformat: string (nullable = true)\n",
      " |    |    |-- receiver: struct (nullable = true)\n",
      " |    |    |    |-- receiverorganization: string (nullable = true)\n",
      " |    |    |    |-- receivertype: string (nullable = true)\n",
      " |    |    |-- reportduplicate: string (nullable = true)\n",
      " |    |    |-- reporttype: string (nullable = true)\n",
      " |    |    |-- safetyreportid: string (nullable = true)\n",
      " |    |    |-- safetyreportversion: string (nullable = true)\n",
      " |    |    |-- sender: struct (nullable = true)\n",
      " |    |    |    |-- senderorganization: string (nullable = true)\n",
      " |    |    |    |-- sendertype: string (nullable = true)\n",
      " |    |    |-- serious: string (nullable = true)\n",
      " |    |    |-- seriousnesscongenitalanomali: string (nullable = true)\n",
      " |    |    |-- seriousnessdeath: string (nullable = true)\n",
      " |    |    |-- seriousnessdisabling: string (nullable = true)\n",
      " |    |    |-- seriousnesshospitalization: string (nullable = true)\n",
      " |    |    |-- seriousnesslifethreatening: string (nullable = true)\n",
      " |    |    |-- seriousnessother: string (nullable = true)\n",
      " |    |    |-- transmissiondate: string (nullable = true)\n",
      " |    |    |-- transmissiondateformat: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "049be9a2-e5cc-47a5-a2ea-173ea1dc29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_results = json_data.select(explode(F.col(\"results\")).alias(\"exploded_results\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03b7f963-d4ec-4a13-9495-659f7e9db84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631ea3b-0e4d-4a6a-a23f-4c94b93d9229",
   "metadata": {},
   "source": [
    "### Converting Nested JSON Data into Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd457f53-4b34-45de-81a5-8f07c7cb7704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_data.withColumn(\"keys\", F.json_object_keys(temp_data.exploded_array)).show()\n",
    "keys = exploded_results.select(F.col(\"exploded_results.*\")).columns\n",
    "keys = [\"exploded_results.\"+str(i) for i in keys]\n",
    "all_keys.extend(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ff4b7c3-3906-4992-899e-79440a8b08d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_keys = exploded_results.select(F.col(\"exploded_results.patient.*\")).columns\n",
    "patient_keys = [\"exploded_results.patient.\"+str(i) for i in patient_keys]\n",
    "all_keys.extend(patient_keys)\n",
    "updated_data = exploded_results.select(all_keys)\n",
    "updated_data = updated_data.drop(F.col(\"patient\"))\n",
    "all_keys = updated_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8a11e61-c0d1-47fe-83ae-a83ca13d5db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_data = updated_data.select(all_keys)\\\n",
    "            .withColumn(\"explode_drug\",F.explode(F.col(\"drug\")))\n",
    "drug_keys = updated_data.select(F.col(\"explode_drug.*\")).columns\n",
    "drug_keys = [\"explode_drug.\"+i for i in drug_keys]\n",
    "all_keys.extend(drug_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8af7a00-c33e-4d0e-9200-fad0289a93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_data = updated_data.select(all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee756ce5-caf4-4682-8b59-b228a72901ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_data = updated_data.drop(*['authoritynumb','duplicate','reportduplicate','patientagegroup','patientweight','summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e051b646-f98c-43e2-b4fc-8e8009cc0c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_data = updated_data.where(F.col(\"drugindication\") != 'NULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf1916c7-2990-4d4c-bb87-3dd97b3c5e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_data = updated_data.where(F.col(\"drugindication\") != \"Product used for unknown indication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7145ec60-eaa1-4c47-807f-b56d9bf7ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = updated_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "725d9384-629c-4616-bae9-1cf63bb6e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = updated_data.columns\n",
    "updated_data = updated_data.select(all_keys)\\\n",
    "            .withColumn(\"explode_reaction\",F.explode(F.col(\"reaction\")))\n",
    "all_keys = updated_data.columns\n",
    "reaction_keys = updated_data.select(F.col(\"explode_reaction.*\")).columns\n",
    "reaction_keys = [\"explode_reaction.\"+i for i in reaction_keys]\n",
    "all_keys.extend(reaction_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de326f81-78d5-466b-9f6a-526833875a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_data = updated_data.select(all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9748cd8-8e3e-45a1-bc96-0f53e8a4dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_data = updated_data.where(F.col(\"reactionmeddrapt\") != 'NULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f333a39e-efce-4abd-8ce5-7b4449470c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_data = updated_data.select([\"seriousnessdeath\",\"seriousnesslifethreatening\",\"seriousnesshospitalization\",\"seriousnessdisabling\",\"seriousnesscongenitalanomali\",\"seriousnessother\",\"patientonsetage\",\"reactionmeddrapt\",\"reactionoutcome\",\"drugindication\",\"activesubstance.activesubstancename\",\"medicinalproduct\",\"openfda.route\",\"openfda.brand_name\",\"openfda.generic_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5dd3c01-25d7-43c8-9e75-5b5ad0fe6437",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_data = updated_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd34eefc-4038-4441-85c9-a3a3b97c2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = updated_data.where(F.col(\"reactionmeddrapt\") != 'Off label use')\\\n",
    "                            .where(F.col(\"drugindication\") != 'Off label use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "238115b9-b8ff-4c4d-916f-aa364ef1e9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [\"activesubstancename\",\"medicinalproduct\",\"drugindication\",\"reactionmeddrapt\"]:\n",
    "    grouped_data = data.groupby(F.col(c)).count()\n",
    "    filtered_data = grouped_data.orderBy(\"count\",ascending=False).limit(10)\n",
    "    data = data.join(filtered_data.select(c),on=c, how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca27277-61fe-4554-868b-5a1f80e3da1e",
   "metadata": {},
   "source": [
    "## Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a4bf045-1ee3-45e0-914a-b5476356696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.functions import vector_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2b055e3-88be-4a75-a241-ddf7b2f14023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reactionmeddrapt: string (nullable = true)\n",
      " |-- drugindication: string (nullable = true)\n",
      " |-- medicinalproduct: string (nullable = true)\n",
      " |-- activesubstancename: string (nullable = true)\n",
      " |-- seriousnessdeath: string (nullable = true)\n",
      " |-- seriousnesslifethreatening: string (nullable = true)\n",
      " |-- seriousnesshospitalization: string (nullable = true)\n",
      " |-- seriousnessdisabling: string (nullable = true)\n",
      " |-- seriousnesscongenitalanomali: string (nullable = true)\n",
      " |-- seriousnessother: string (nullable = true)\n",
      " |-- patientonsetage: string (nullable = true)\n",
      " |-- reactionoutcome: string (nullable = true)\n",
      " |-- route: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- brand_name: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- generic_name: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53920fef-e979-4b5e-adb8-bc2c0f9652b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indexer_list = []\n",
    "for c in [\"activesubstancename\",\"medicinalproduct\",\"drugindication\"]:\n",
    "    indexer = StringIndexer(inputCol=c, outputCol=\"op_{}\".format(c))\n",
    "    indexer_list.append(indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8efa62b5-9b21-466e-8d4f-7e147c66a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=indexer_list)\n",
    "pipeline_model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9734f0c-d295-4fe8-887c-7aeceede26e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pipeline_model.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b9e9ac-b7aa-41df-8fe8-af8314f06009",
   "metadata": {},
   "source": [
    "## Random Forest Predicting reactionoutcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67081ae3-a126-4d91-b3dc-5f6b0098504b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474ff02-1cb6-4670-9240-99eb501ee8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c4edff3a-caa5-47e9-8734-cc704848220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "data = data.withColumn(\"op_medicinalproduct\",F.col(\"op_medicinalproduct\").cast(IntegerType()))\\\n",
    "    .withColumn(\"op_activesubstancename\",F.col(\"op_activesubstancename\").cast(IntegerType()))\\\n",
    "    .withColumn(\"op_drugindication\",F.col(\"op_drugindication\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnessdeath\",F.col(\"seriousnessdeath\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnesslifethreatening\",F.col(\"seriousnesslifethreatening\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnesshospitalization\",F.col(\"seriousnesshospitalization\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnessdisabling\",F.col(\"seriousnessdisabling\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnesscongenitalanomali\",F.col(\"seriousnesscongenitalanomali\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnessother\",F.col(\"seriousnessother\").cast(IntegerType()))\\\n",
    "    .withColumn(\"patientonsetage\",F.col(\"patientonsetage\").cast(IntegerType()))\\\n",
    "    .withColumn(\"reactionoutcome\",F.col(\"reactionoutcome\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "674c5b2b-074c-4640-9e2f-07c8b002a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.drop(*[\"route\",\"brand_name\",\"generic_name\",\"reactionoutcome\",\"drugindication\",\"medicinalproduct\",\"activesubstancename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98135c0-d389-4924-bd1f-5726fbd11df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a09d3c14-bb48-46a2-b45e-bd57245b7a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"op_activesubstancename\", \"op_medicinalproduct\", \"op_drugindication\",\"seriousnessdeath\",\"seriousnesslifethreatening\",\"seriousnesshospitalization\",\"seriousnessdisabling\",\"seriousnesscongenitalanomali\",\"seriousnessother\",\"patientonsetage\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "data = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1085184a-f37a-406d-a7a6-9f7486404b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.select([\"op_activesubstancename\", \"op_medicinalproduct\", \"op_drugindication\",\"features\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e2d8509-1345-422e-a2f2-d9bc10bc62f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"reactionoutcome\", outputCol=\"label\").fit(data)\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"feature\", outputCol=\"features\", maxCategories=4).fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c412748d-aa6c-409f-965f-97a5f2f5d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "efb337f1-a11f-4333-8c64-62f42fa37938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e29c0fe6-7756-4ccb-9be6-fb30d63d5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainingData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0dbaaf8f-b865-4a3e-bc5b-026871060a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, rf])\n",
    "# pipeline = Pipeline(stages=indexer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b8e0ddf-7084-442e-ba33-653f0462ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7eba7fe3-99be-4488-9a16-50823173699a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------------------+\n",
      "|prediction|reactionoutcome|            features|\n",
      "+----------+---------------+--------------------+\n",
      "|       5.0|              5|[0.0,0.0,7.0,1.0,...|\n",
      "|       5.0|              5|[0.0,0.0,2.0,1.0,...|\n",
      "|       5.0|              5|[0.0,0.0,2.0,1.0,...|\n",
      "|       5.0|              5|[0.0,0.0,2.0,1.0,...|\n",
      "|       5.0|              5|[0.0,0.0,2.0,1.0,...|\n",
      "+----------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd86c5d9-4c1c-4f8a-9fd0-cdb00a031d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.071775\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9919f447-f032-438a-bb21-6405e148c1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90731ba0-b010-4889-825a-ff2e68078c42",
   "metadata": {},
   "source": [
    "## Random Forest for predicting Predicting Reaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae2339e2-3477-4820-aa63-0773a8f17519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "data = data.withColumn(\"op_medicinalproduct\",F.col(\"op_medicinalproduct\").cast(IntegerType()))\\\n",
    "    .withColumn(\"op_activesubstancename\",F.col(\"op_activesubstancename\").cast(IntegerType()))\\\n",
    "    .withColumn(\"op_drugindication\",F.col(\"op_drugindication\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnessdeath\",F.col(\"seriousnessdeath\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnesslifethreatening\",F.col(\"seriousnesslifethreatening\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnesshospitalization\",F.col(\"seriousnesshospitalization\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnessdisabling\",F.col(\"seriousnessdisabling\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnesscongenitalanomali\",F.col(\"seriousnesscongenitalanomali\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnessother\",F.col(\"seriousnessother\").cast(IntegerType()))\\\n",
    "    .withColumn(\"patientonsetage\",F.col(\"patientonsetage\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17b441b1-c014-4400-9385-b121cfd6bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.drop(*[\"route\",\"brand_name\",\"generic_name\",\"reactionoutcome\",\"drugindication\",\"medicinalproduct\",\"activesubstancename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b836ec5-7de9-40c5-aecb-a3d91e7bfd25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d77b29f2-c7f4-43f5-964b-9bf7c64a0f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"op_activesubstancename\", \"op_medicinalproduct\", \"op_drugindication\",\"seriousnessdeath\",\"seriousnesslifethreatening\",\"seriousnesshospitalization\",\"seriousnessdisabling\",\"seriousnesscongenitalanomali\",\"seriousnessother\",\"patientonsetage\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "data = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ecced2d8-afaf-41f1-b1c9-dfd21a55beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.select([\"op_activesubstancename\", \"op_medicinalproduct\", \"op_drugindication\",\"features\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2ec18a83-9aa3-470c-80e3-3dedc278e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"reactionmeddrapt\", outputCol=\"indexedLabel\").fit(data)\n",
    "# indexer_list.append(labelIndexer)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "# featureIndexer =\\\n",
    "#     VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b10b2f37-9c60-45cb-af0b-239f404f7f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b1c6bf5f-4d8a-4ac0-bcc6-8323f03c3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", numTrees=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e43aa1fc-e33b-4751-8adc-19731c71707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ff4ca536-d7b2-47ba-8e5f-1336ca5dfb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "# pipeline = Pipeline(stages=indexer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f8210a97-f084-4838-87b0-10308cbf86d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6a61b3bd-8f77-4db4-a908-e6a43e376491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------------------+\n",
      "|      predictedLabel|reactionmeddrapt|            features|\n",
      "+--------------------+----------------+--------------------+\n",
      "|Systemic lupus er...|        Alopecia|[0.0,0.0,2.0,0.0,...|\n",
      "|Systemic lupus er...|        Alopecia|[0.0,0.0,2.0,0.0,...|\n",
      "|Systemic lupus er...|        Alopecia|[0.0,0.0,5.0,0.0,...|\n",
      "|Systemic lupus er...|        Alopecia|[0.0,0.0,5.0,0.0,...|\n",
      "|           Synovitis|        Alopecia|[6.0,5.0,3.0,0.0,...|\n",
      "+--------------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"reactionmeddrapt\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d8cc63d-c348-41d6-81ad-fceb68a1a0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.892457\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "22cc4dfe-5b05-4aaa-b073-0226b66cab0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassificationModel: uid=RandomForestClassifier_c1adb11429d7, numTrees=100, numClasses=10, numFeatures=10\n"
     ]
    }
   ],
   "source": [
    "rfModel = model.stages[2]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b8ef03-0eed-4042-84eb-ef503bc0de15",
   "metadata": {},
   "source": [
    "## Naive Bayes for predciting reaction outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "475c6d75-532c-4513-9899-4eabfff49c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8be3481-23cf-49c9-9a5b-484c52b2cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "data = data.withColumn(\"op_medicinalproduct\",F.col(\"op_medicinalproduct\").cast(IntegerType()))\\\n",
    "    .withColumn(\"op_activesubstancename\",F.col(\"op_activesubstancename\").cast(IntegerType()))\\\n",
    "    .withColumn(\"op_drugindication\",F.col(\"op_drugindication\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnessdeath\",F.col(\"seriousnessdeath\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnesslifethreatening\",F.col(\"seriousnesslifethreatening\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnesshospitalization\",F.col(\"seriousnesshospitalization\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnessdisabling\",F.col(\"seriousnessdisabling\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnesscongenitalanomali\",F.col(\"seriousnesscongenitalanomali\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnessother\",F.col(\"seriousnessother\").cast(IntegerType()))\\\n",
    "    .withColumn(\"patientonsetage\",F.col(\"patientonsetage\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f53892de-dd34-4426-b377-eedacfc0f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.drop(*[\"route\",\"brand_name\",\"generic_name\",\"reactionoutcome\",\"drugindication\",\"medicinalproduct\",\"activesubstancename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1b675f-f6e4-453f-9e04-098d64def2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d5e09b0-191c-4df2-928c-4ac1589ace13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"op_activesubstancename\", \"op_medicinalproduct\", \"op_drugindication\",\"seriousnessdeath\",\"seriousnesslifethreatening\",\"seriousnesshospitalization\",\"seriousnessdisabling\",\"seriousnesscongenitalanomali\",\"seriousnessother\",\"patientonsetage\"],\n",
    "    outputCol=\"feature\"\n",
    ")\n",
    "\n",
    "data = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1e260fc-21a6-4ccd-b059-70e8f268e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.select([\"op_activesubstancename\", \"op_medicinalproduct\", \"op_drugindication\",\"features\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a1f5a229-e44c-46da-a161-9386031c6cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"reactionoutcome\", outputCol=\"label\").fit(data)\n",
    "# indexer_list.append(labelIndexer)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"feature\", outputCol=\"features\", maxCategories=4).fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f3837085-96d3-474c-90e3-7c44e63492d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ee86577-95ea-4615-9f70-2bc290c5ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\", modelType=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9d482bcb-8090-45ab-a536-ab79c977284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nb.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "809234d6-c914-43da-8d5d-da897ad89d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32a963dd-d3f0-46bc-bb80-d7932d0bdee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43911203-5a93-4d28-a880-23bebe64b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, nb, labelConverter])\n",
    "# pipeline = Pipeline(stages=indexer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3fd0b9a5-8b89-4326-82b4-36c3fc661efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6bee33ef-1019-48f5-9f95-913c5d8787bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+--------------------+\n",
      "|predictedLabel|reactionoutcome|            features|\n",
      "+--------------+---------------+--------------------+\n",
      "|             2|              5|[0.0,0.0,7.0,0.0,...|\n",
      "|             5|              5|[0.0,0.0,2.0,0.0,...|\n",
      "|             5|              5|[0.0,0.0,2.0,0.0,...|\n",
      "|             5|              5|[0.0,0.0,2.0,0.0,...|\n",
      "|             5|              5|[0.0,0.0,2.0,0.0,...|\n",
      "+--------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"reactionoutcome\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3e5c0eda-d058-4006-8cd5-3db483a3766b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.2\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b50a078-dfd8-4225-a145-33e680cc2236",
   "metadata": {},
   "source": [
    "## MLP for prediction reaction outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63e1653a-79fa-4b93-9cca-e2aded0e0874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "data = data.withColumn(\"op_medicinalproduct\",F.col(\"op_medicinalproduct\").cast(IntegerType()))\\\n",
    "    .withColumn(\"op_activesubstancename\",F.col(\"op_activesubstancename\").cast(IntegerType()))\\\n",
    "    .withColumn(\"op_drugindication\",F.col(\"op_drugindication\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnessdeath\",F.col(\"seriousnessdeath\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnesslifethreatening\",F.col(\"seriousnesslifethreatening\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnesshospitalization\",F.col(\"seriousnesshospitalization\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnessdisabling\",F.col(\"seriousnessdisabling\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnesscongenitalanomali\",F.col(\"seriousnesscongenitalanomali\").cast(IntegerType()))\\\n",
    "    .withColumn(\"seriousnessother\",F.col(\"seriousnessother\").cast(IntegerType()))\\\n",
    "    .withColumn(\"patientonsetage\",F.col(\"patientonsetage\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb6aee26-63b3-492e-a8b8-7a36819c8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"op_activesubstancename\", \"op_medicinalproduct\", \"op_drugindication\",\"seriousnessdeath\",\"seriousnesslifethreatening\",\"seriousnesshospitalization\",\"seriousnessdisabling\",\"seriousnesscongenitalanomali\",\"seriousnessother\",\"patientonsetage\"],\n",
    "    outputCol=\"feature\"\n",
    ")\n",
    "\n",
    "data = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d686736-a97b-4a09-9e3f-4589341b4969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"reactionoutcome\", outputCol=\"label\").fit(data)\n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"feature\", outputCol=\"features\", maxCategories=4).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aae718ce-0e1e-48ad-a0b9-4ffc3dae4c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = labelIndexer.transform(data)\n",
    "data = featureIndexer.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "303d21e3-56a8-4970-b129-6772a25353ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13b30e9d-a3f0-4b1d-8f7c-9b37724cda01",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8a668f31-e381-4916-9896-f1ebe1835b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultilayerPerceptronClassificationModel: uid=MultilayerPerceptronClassifier_e7791adbd601, numLayers=3, numClasses=6, numFeatures=10"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(layers=[10, 20, 6], seed=123)\n",
    "mlp.setMaxIter(500)\n",
    "mlp.getMaxIter()\n",
    "mlp.getBlockSize()\n",
    "mlp.setBlockSize(1)\n",
    "mlp.getBlockSize()\n",
    "model = mlp.fit(trainingData)\n",
    "model.setFeaturesCol(\"features\")\n",
    "# testData.head().features\n",
    "# trainingData.head().features\n",
    "# model.predict(testData.head().features)\n",
    "# model.predictRaw(testData.head().features)\n",
    "# model.predictProbability(testData.head().features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5a9f253-275c-4f9a-b31f-1eaf516b0212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 0.0, 0.0, 0.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predictProbability(testData.head().features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "267c2192-023e-4e5a-a901-b18ac8ab7d66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "235005d1-760c-495d-8721-3988af80353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+\n",
      "|            features|prediction|label|\n",
      "+--------------------+----------+-----+\n",
      "|[0.0,0.0,7.0,0.0,...|       0.0|  0.0|\n",
      "|[0.0,0.0,2.0,0.0,...|       0.0|  0.0|\n",
      "|[0.0,0.0,2.0,0.0,...|       0.0|  1.0|\n",
      "|[0.0,0.0,2.0,0.0,...|       0.0|  0.0|\n",
      "|[0.0,0.0,8.0,0.0,...|       0.0|  0.0|\n",
      "+--------------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"features\", \"prediction\",\"label\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aac04343-93b2-4864-b14c-3db31400ca55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.088644\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f20d85-c901-43aa-ab15-14cd02cabf41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "978b5d5d-298f-4e7d-883f-c7ba7a506b83",
   "metadata": {},
   "source": [
    "## MLP Reaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "db34b7eb-8dee-4dc7-b8fa-26f3688c4bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3b0b61a6-1438-44e4-8271-530917e59dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = labelIndexer.transform(data)\n",
    "data = featureIndexer.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "06849e1d-a33e-4c97-9e42-2e88355f62b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f15e08ad-5ba3-4ed6-bc8e-bdb9db4e321c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0, 0.0, 7.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 44.0])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData.head().features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ba48c6c5-97c5-4469-a959-a09ecc4e24e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1491.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 943.0 failed 1 times, most recent failure: Lost task 0.0 in stage 943.0 (TID 836) (Akshay executor driver): java.lang.RuntimeException: Labels MUST be in [0, 1), but got 1.0\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.project_doConsume_1$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1296)\r\n\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\r\n\tat org.apache.spark.mllib.optimization.LBFGS.optimizeWithLossReturned(LBFGS.scala:154)\r\n\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:855)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.$anonfun$train$1(MultilayerPerceptronClassifier.scala:233)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:185)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:94)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.RuntimeException: Labels MUST be in [0, 1), but got 1.0\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.project_doConsume_1$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m mlp\u001b[38;5;241m.\u001b[39msetBlockSize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m mlp\u001b[38;5;241m.\u001b[39mgetBlockSize()\n\u001b[1;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainingData\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39msetFeaturesCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1491.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 943.0 failed 1 times, most recent failure: Lost task 0.0 in stage 943.0 (TID 836) (Akshay executor driver): java.lang.RuntimeException: Labels MUST be in [0, 1), but got 1.0\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.project_doConsume_1$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1296)\r\n\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\r\n\tat org.apache.spark.mllib.optimization.LBFGS.optimizeWithLossReturned(LBFGS.scala:154)\r\n\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:855)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.$anonfun$train$1(MultilayerPerceptronClassifier.scala:233)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:185)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:94)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.RuntimeException: Labels MUST be in [0, 1), but got 1.0\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.project_doConsume_1$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage43.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(layers=[10, 20, 10], seed=123)\n",
    "mlp.setMaxIter(500)\n",
    "mlp.getMaxIter()\n",
    "mlp.getBlockSize()\n",
    "mlp.setBlockSize(1)\n",
    "mlp.getBlockSize()\n",
    "model = mlp.fit(trainingData)\n",
    "model.setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf4867-e19f-4500-b139-fa29886cd68b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
